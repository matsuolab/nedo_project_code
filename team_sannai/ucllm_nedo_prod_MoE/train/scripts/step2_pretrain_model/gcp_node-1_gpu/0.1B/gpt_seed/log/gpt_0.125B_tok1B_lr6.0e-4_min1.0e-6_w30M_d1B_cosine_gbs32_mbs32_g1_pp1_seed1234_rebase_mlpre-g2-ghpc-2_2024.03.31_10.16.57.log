[2024-03-31 10:17:04,929] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 10:17:11,813] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2024-03-31 10:17:11,821] [INFO] [runner.py:570:main] cmd = /home/ext_012beam_gmail_com/miniconda3/envs/.venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=6003 --enable_each_rank_log=None /home/ext_012beam_gmail_com/ucllm_nedo_prod_MoE/train/Megatron-DeepSpeed-MoE/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --init-method-std 0.02 --lr-decay-tokens 1000000000 --lr-warmup-tokens 16384000 --micro-batch-size 32 --exit-duration-in-mins 30000000 --global-batch-size 32 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 512 --max-position-embeddings 512 --train-tokens 1000000000 --train-samples 11197430 --lr 6.0e-4 --min-lr 1.0e-6 --lr-decay-style cosine --split 949,50,1 --log-interval 10 --eval-interval 100 --eval-iters 10 --save-interval 1000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --seed 1234 --load ./gpt_seed/checkpoint/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase --save ./gpt_seed/checkpoint/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase --no-async-tensor-model-parallel-allreduce --use-flash-attn-v2 --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir ./gpt_seed/tensorboard/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase_mlpre-g2-ghpc-2_2024.03.31_10.16.57 --untie-embeddings-and-output-weights --log-optimizer-states-to-tensorboard --tokenizer-type HFTokenizer --tokenizer-model mistralai/Mixtral-8x7B-v0.1 --data-path 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_1/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_2/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_3/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_4/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_5/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_6/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_7/_text_document 1 /persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_8/_text_document --data-impl mmap --deepspeed --deepspeed_config ./gpt_seed/deepspeed_config/ds_config_gbs32_mbs32_log10_zero0.json --zero-stage 0 --pipeline-model-parallel-size 1
[2024-03-31 10:17:13,821] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-31 10:17:16,369] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-03-31 10:17:16,369] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-03-31 10:17:16,370] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-03-31 10:17:16,370] [INFO] [launch.py:163:main] dist_world_size=1
[2024-03-31 10:17:16,370] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
/home/ext_012beam_gmail_com/miniconda3/envs/.venv/bin/python: can't open file '/home/ext_012beam_gmail_com/ucllm_nedo_prod_MoE/train/Megatron-DeepSpeed-MoE/pretrain_gpt.py': [Errno 2] No such file or directory
[2024-03-31 10:17:17,379] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 12650
[2024-03-31 10:17:17,380] [ERROR] [launch.py:321:sigkill_handler] ['/home/ext_012beam_gmail_com/miniconda3/envs/.venv/bin/python', '-u', '/home/ext_012beam_gmail_com/ucllm_nedo_prod_MoE/train/Megatron-DeepSpeed-MoE/pretrain_gpt.py', '--local_rank=0', '--override-opt_param-scheduler', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.02', '--lr-decay-tokens', '1000000000', '--lr-warmup-tokens', '16384000', '--micro-batch-size', '32', '--exit-duration-in-mins', '30000000', '--global-batch-size', '32', '--num-layers', '12', '--hidden-size', '768', '--num-attention-heads', '12', '--seq-length', '512', '--max-position-embeddings', '512', '--train-tokens', '1000000000', '--train-samples', '11197430', '--lr', '6.0e-4', '--min-lr', '1.0e-6', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '100', '--eval-iters', '10', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--fp16', '--seed', '1234', '--load', './gpt_seed/checkpoint/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase', '--save', './gpt_seed/checkpoint/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--use-flash-attn-v2', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', './gpt_seed/tensorboard/gpt_0.125B_tok1B_lr6.0e-4_min1.0e-6_w30M_d1B_cosine_gbs32_mbs32_g1_pp1_seed1234_rebase_mlpre-g2-ghpc-2_2024.03.31_10.16.57', '--untie-embeddings-and-output-weights', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'HFTokenizer', '--tokenizer-model', 'mistralai/Mixtral-8x7B-v0.1', '--data-path', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_1/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_2/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_3/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_4/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_5/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_6/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_7/_text_document', '1', '/persistentshare/storage/team_sannai/team_la/seed/dataset3/preprocessed_8/_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', './gpt_seed/deepspeed_config/ds_config_gbs32_mbs32_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '1'] exits with return code = 2
