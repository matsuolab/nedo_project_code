model:
  model: OsakanaTeishoku/mixtral_small_dummy
  tokenizer: u-10bei/JINIAC_tokenizer_v0.2
  use_cache: False
  max_length: 2048

train: # huggingfaceのTrainingArgumentsで利用
  output_dir: /persistentshare/storage/team_nakamura/member/shiraishi/dev_0420
  evaluation_strategy: steps
  logging_strategy: steps
  save_strategy: "no"
  learning_rate: 2e-5
  lr_scheduler_type: cosine
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: True
  weight_decay: 0.01 
  adam_beta1: 0.9
  adam_beta2: 0.95
  warmup_ratio: 0.1 
  #optim: adamw_torch 
  fp16: False
  bf16: True
  dataloader_num_workers: 4
  eval_steps: 10
  #save_steps: 100
  logging_steps: 1
  run_name: test_0420_z3_modify_config_torchrun
  save_total_limit: 2
  save_on_each_node: False
  neftune_noise_alpha: 5 
  #torch_compile: True
  deepspeed: ~/pretrain/config/ds_config.json
  report_to: wandb
  
seed: 42

dataset:
  path: /persistentshare/storage/team_nakamura/member/shiraishi/data/0313wiki_1000000_with_python.jsonl
  # subset: chunked #!!null
  split: train