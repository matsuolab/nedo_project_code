{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6OalFLmTN-a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyvjxWL3-BUL"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install accelerate>=0.21.0\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A700NIcTAHJb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import concatenate_datasets\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"\"\"your model name\"\"\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        use_fast=True,\n",
        "        trust_remote_code=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "wwMSnJafgc9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge dolly-ja"
      ],
      "metadata": {
        "id": "a-sfIBvD--QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_name =\"kunishou/databricks-dolly-15k-ja\"\n",
        "dataset = load_dataset(dataset_name)['train']\n",
        "df = pd.DataFrame(dataset)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ETCxrMcYr13h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset\n",
        "system_message = f\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n\"\n",
        "response_template = \"\\n\\n### 応答:\\n\"\n",
        "input_template = \"\\n\\n### 入力:\\n\"\n",
        "\n",
        "def merge(examples):\n",
        "    if pd.isna(examples['input']) or examples['input'] == '':\n",
        "        return system_message + examples['instruction'] + response_template + str(examples['output'])\n",
        "    else:\n",
        "        return system_message + examples['instruction'] + input_template + examples['input'] + response_template + str(examples['output'])\n",
        "\n",
        "df['text'] = df.apply(merge, axis=1)"
      ],
      "metadata": {
        "id": "_FkSSTgMrs3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path=f\"/content/drive/MyDrive/GENIAC/\" + \"dolly-15k-jp-merged.jsonl\"\n",
        "import json\n",
        "with open(output_path, 'w') as f:\n",
        "    for id, row in df.iterrows():\n",
        "        f.write(json.dumps({\"text\": row['text']}, ensure_ascii=False)+\"\\n\")\n",
        "dataset = load_dataset(\"json\", data_files=output_path)"
      ],
      "metadata": {
        "id": "X1NBpo7fqvxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "name=output_path\n",
        "dataset = load_dataset(\"json\", data_files=name)\n",
        "no_filter = dataset.num_rows\n",
        "print(f\"Number of rows in {dataset.num_rows}\")\n",
        "dataset_filltered = dataset.filter(lambda x: len(tokenizer.encode(x['text'])) < 2045)\n",
        "print(f\"Number of filltered rows in {dataset_filltered .num_rows}\")\n",
        "if no_filter != dataset_filltered.num_rows:\n",
        "    output_path=f\"/content/drive/MyDrive/GENIAC/\" + name.split(\"/\")[-1].split('.')[0]  + \"-filtered.jsonl\"\n",
        "    dataset_filltered = pd.DataFrame(dataset_filltered['train'])\n",
        "    import json\n",
        "    with open(output_path, 'w') as f:\n",
        "        for id, row in dataset_filltered.iterrows():\n",
        "            f.write(json.dumps({\"text\": row['text']}, ensure_ascii=False)+\"\\n\")\n",
        "    dataset = load_dataset(\"json\", data_files=output_path)\n",
        "    print(dataset[\"train\"][0:10])\n",
        "else:\n",
        "    print(f\"No filltered rows in {dataset.num_rows}\")"
      ],
      "metadata": {
        "id": "XsRJK-fbsqzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##oasst2-32k-ja"
      ],
      "metadata": {
        "id": "R3J2m918VeWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset_name =\"llm-jp/oasst2-33k-ja\"\n",
        "Oasst_21k_dataset = load_dataset(dataset_name)['train']\n",
        "df = pd.DataFrame(Oasst_21k_dataset)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "4uRA_KLhrKbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = f\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n\"\n",
        "human_template = \"\\n\\n### 指示:\\n\"\n",
        "response_template = \"\\n\\n### 応答:\\n\"\n",
        "\n",
        "def merge(examples):\n",
        "    messages = system_message\n",
        "    for i in range(len(examples['conversations'])):\n",
        "        if examples['conversations'][i]['role'] == \"user\":\n",
        "            if i!=0:\n",
        "                messages += human_template\n",
        "            messages += examples['conversations'][i]['content']\n",
        "        elif i < len(examples['conversations']) - 1:\n",
        "            messages += response_template + examples['conversations'][i]['content'] +\"</s>\"\n",
        "        else:\n",
        "            messages += response_template + examples['conversations'][i]['content']\n",
        "    return messages\n",
        "\n",
        "df['text'] = df.apply(merge, axis=1)\n",
        "df['conversations'] = ''\n",
        "df['text'].iloc[1]"
      ],
      "metadata": {
        "id": "G6-pg7FYVvxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path=f\"/content/drive/MyDrive/GENIAC/\" + \"oasst2-33k-en-merged-sp.jsonl\"\n",
        "import json\n",
        "with open(output_path, 'w') as f:\n",
        "    for id, row in df.iterrows():\n",
        "        f.write(json.dumps({\"text\": row['text']}, ensure_ascii=False)+\"\\n\")\n",
        "dataset = load_dataset(\"json\", data_files=output_path)\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "JxJHFVuN3pmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "name=output_path\n",
        "dataset = load_dataset(\"json\", data_files=name)\n",
        "no_filter = dataset.num_rows\n",
        "print(f\"Number of rows in {dataset.num_rows}\")\n",
        "dataset_filltered = dataset.filter(lambda x: len(tokenizer.encode(x['text'])) < 2045)\n",
        "print(f\"Number of filltered rows in {dataset_filltered .num_rows}\")\n",
        "if no_filter != dataset_filltered.num_rows:\n",
        "    output_path=f\"/content/drive/MyDrive/GENIAC/\" + name.split(\"/\")[-1].split('.')[0]  + \"-filtered.jsonl\"\n",
        "    dataset_filltered = pd.DataFrame(dataset_filltered['train'])\n",
        "    import json\n",
        "    with open(output_path, 'w') as f:\n",
        "        for id, row in dataset_filltered.iterrows():\n",
        "            f.write(json.dumps({\"text\": row['text']}, ensure_ascii=False)+\"\\n\")\n",
        "    dataset = load_dataset(\"json\", data_files=output_path)\n",
        "    print(dataset[\"train\"][0:10])\n",
        "else:\n",
        "    print(f\"No filltered rows in {dataset.num_rows}\")"
      ],
      "metadata": {
        "id": "D6Ah-uBQ3wb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}